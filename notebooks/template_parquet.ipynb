{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access {{ dataset_name }} data in Parquet\n",
    "\n",
    "A jupyter notebook to show how to access and plot {{ dataset_name }} data available as a [Parquet](https://parquet.apache.org) dataset on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"dataset_parquet_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install/Update packages and Load common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once, then restart session if needed\n",
    "!pip install uv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Get the current directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Check if requirements.txt exists in the current directory\n",
    "local_requirements = os.path.join(current_dir, 'requirements.txt')\n",
    "if os.path.exists(local_requirements):\n",
    "    requirements_path = local_requirements\n",
    "else:\n",
    "    # Fall back to the online requirements.txt file\n",
    "    requirements_path = 'https://raw.githubusercontent.com/aodn/aodn_cloud_optimised/main/notebooks/requirements.txt'\n",
    "\n",
    "# Install packages using uv and the determined requirements file\n",
    "if is_colab():  # For Google Colab\n",
    "    import xarray as xr\n",
    "    xr.set_options(display_style='text')\n",
    "    os.system(f'uv pip install --system -r {requirements_path}')\n",
    "    os.system('uv pip install --system pyopenssl --upgrade')\n",
    "elif 'jupyter' in platform.uname().node:  # For Nectar Instance https://jupyterhub.rc.nectar.org.au\n",
    "    os.system(f'uv pip install --system -r {requirements_path}')\n",
    "else: # If running locallly\n",
    "    os.system('uv venv')\n",
    "    os.system(f'uv pip install -r {requirements_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import importlib.util\n",
    "from packaging.version import Version, InvalidVersion\n",
    "\n",
    "DATAQUERY_PATH = \"DataQuery.py\"\n",
    "DATAQUERY_URL = \"https://raw.githubusercontent.com/aodn/aodn_cloud_optimised/main/aodn_cloud_optimised/lib/DataQuery.py\"\n",
    "\n",
    "\n",
    "def load_dataquery():\n",
    "    remote_version, remote_code = get_remote_version_and_code()\n",
    "    if remote_version is None:\n",
    "        print(\"âŒ Remote file does not contain a valid __version__, skipping update.\")\n",
    "        return\n",
    "\n",
    "    local_version = get_local_version()\n",
    "    if local_version is None:\n",
    "        print(\"âš ï¸ Local file has no version or is missing. Downloading remote file.\")\n",
    "        write_dataquery(remote_code)\n",
    "    elif remote_version > local_version:\n",
    "        print(f\"ðŸ”„ Updating: local version {local_version} < remote version {remote_version}\")\n",
    "        write_dataquery(remote_code)\n",
    "    else:\n",
    "        print(f\"âœ… Local version {local_version} is up to date (remote: {remote_version})\")\n",
    "\n",
    "\n",
    "def get_local_version():\n",
    "    if not os.path.exists(DATAQUERY_PATH):\n",
    "        return None\n",
    "    try:\n",
    "        spec = importlib.util.spec_from_file_location(\"DataQuery\", DATAQUERY_PATH)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        version_str = getattr(module, \"__version__\", None)\n",
    "        return Version(version_str) if version_str else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading local version: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_remote_version_and_code():\n",
    "    try:\n",
    "        response = requests.get(DATAQUERY_URL)\n",
    "        response.raise_for_status()\n",
    "        code = response.text\n",
    "        match = re.search(r'^__version__\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', code, re.MULTILINE)\n",
    "        if match:\n",
    "            version_str = match.group(1)\n",
    "            return Version(version_str), code\n",
    "        else:\n",
    "            return None, code\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching remote file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def write_dataquery(code):\n",
    "    with open(DATAQUERY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(code)\n",
    "    print(f\"ðŸ“¥ Wrote updated DataQuery.py\")\n",
    "\n",
    "\n",
    "load_dataquery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataQuery import GetAodn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get partition keys\n",
    "Partitioning in Parquet involves organising data files based on the values of one or more columns, known as partition keys. When data is written to Parquet files with partitioning enabled, the files are physically stored in a directory structure that reflects the partition keys. This directory structure makes it easier to retrieve and process specific subsets of data based on the partition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn = GetAodn()\n",
    "dname = f'{dataset_name}.parquet'\n",
    "%time aodn_dataset = aodn.get_dataset(dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn_dataset.dataset.partitioning.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List unique partition values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unique_partition_value = aodn_dataset.get_unique_partition_values('YOUR_PARTITION_KEY')\n",
    "print(list(unique_partition_value)[0:2])  # showing a subset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Spatial Extent of the dataset\n",
    "In this section, we're plotting the polygons where data exists. This helps then with creating a bounding box where there is data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn_dataset.plot_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Temporal Extent of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary to the spatial extent, we're retrieving the minimum and maximum timestamp partition values of the dataset. This is not necessarely accurately representative of the TIME values, as the timestamp partition can be yearly/monthly... but is here to give an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aodn_dataset.get_temporal_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Metadata\n",
    "\n",
    "For all parquet dataset, we create a sidecar file in the root of the dataset named **_common_matadata**. This contains the variable attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = aodn_dataset.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Query and Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TIME and BoundingBox filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = aodn_dataset.get_data(date_start='2022-12-01', date_end='2023-01-01',lat_min=-34, lat_max=-28, lon_min=151, lon_max=160, lat_varname='latitude', lon_varname='longitude')\n",
    "\n",
    "df = pd.read_parquet(dname, engine='pyarrow',filters=filter)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TIME and scalar/number filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = aodn_dataset.get_data(date_start='2006-07-12', date_end='2023-02-05',scalar_filter='YOUR_PARTITION_KEY': 1901740})\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
