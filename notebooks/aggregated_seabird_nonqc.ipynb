{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f77d1ce",
   "metadata": {},
   "source": [
    "## Access Aggregated Seabird Nonqc (Parquet)\n",
    "This Jupyter notebook demonstrates how to access and plot aggregated_seabird_nonqc data, available as a [Parquet](https://parquet.apache.org) dataset stored on S3.\n",
    "\n",
    "ðŸ”— More information about the dataset is available [in the AODN metadata catalogue](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/None).\n",
    "\n",
    "ðŸ“Œ The source of truth for this notebook is maintained on [GitHub](https://github.com/aodn/aodn_cloud_optimised/tree/main/notebooks/aggregated_seabird_nonqc.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb09bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"aggregated_seabird_nonqc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017dc7c",
   "metadata": {},
   "source": [
    "## Install/Update packages and Load common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, importlib.util\n",
    "\n",
    "open('setup.py', 'w').write(requests.get('https://raw.githubusercontent.com/aodn/aodn_cloud_optimised/main/notebooks/setup.py').text)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"setup\", \"setup.py\")\n",
    "setup = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(setup)\n",
    "\n",
    "setup.install_requirements()\n",
    "setup.load_dataquery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253038ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataQuery import GetAodn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0e3a2",
   "metadata": {},
   "source": [
    "# Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb12a96",
   "metadata": {},
   "source": [
    "## Understanding Parquet Partitioning\n",
    "\n",
    "Parquet files can be **partitioned** by one or more columns, which means the data is physically organised into folders based on the values in those columns. This is similar to how databases use indexes to optimise query performance.\n",
    "\n",
    "Partitioning enables **faster filtering**: when you query data using a partitioned column, only the relevant subset of files needs to be readâ€”improving performance significantly.\n",
    "\n",
    "For example, if a dataset is partitioned by `\"site_code\"`, `\"timestamp\"`, and `\"polygon\"`, filtering on `\"site_code\"` allows the system to skip unrelated files entirely.\n",
    "\n",
    "In this notebook, the `GetAodn` class includes built-in methods to efficiently filter data by **time** and **latitude/longitude** using the **timestamp** and **polygon** partitions. Other partitions can be used for filtering via the `scalar_filter`.\n",
    "\n",
    "Any filtering on columns that are **not** partitioned can be significantly slower, as all files may need to be scanned. However, the `GetAodn` class provides a `scalar_filter` method that lets you apply these filters at load timeâ€”before the data is fully readâ€”helping reduce the size of the resulting DataFrame.\n",
    "\n",
    "Once the dataset is loaded, further filtering using Pandas is efficient and flexible.\n",
    "\n",
    "See further below in the notebook for examples of how to filter the data effectively.\n",
    "\n",
    "To view the actual partition columns for this dataset, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91501547",
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn = GetAodn()\n",
    "dname = f'{dataset_name}.parquet'\n",
    "%time aodn_dataset = aodn.get_dataset(dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn_dataset.dataset.partitioning.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e2880",
   "metadata": {},
   "source": [
    "## List unique partition values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39950040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unique_partition_value = aodn_dataset.get_unique_partition_values('YOUR_PARTITION_KEY')\n",
    "print(list(unique_partition_value)[0:2])  # showing a subset only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0ebe0",
   "metadata": {},
   "source": [
    "## Visualise Spatial Extent of the dataset\n",
    "This section plots the polygons representing the areas where data is available. It helps to identify and create a bounding box around the regions containing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aodn_dataset.plot_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b44462",
   "metadata": {},
   "source": [
    "## Get Temporal Extent of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747cc97c",
   "metadata": {},
   "source": [
    "Similary to the spatial extent, we're retrieving the minimum and maximum timestamp partition values of the dataset. This is not necessarely accurately representative of the TIME values, as the timestamp partition can be yearly/monthly... but is here to give an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aodn_dataset.get_temporal_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bd4dd",
   "metadata": {},
   "source": [
    "## Read Metadata\n",
    "\n",
    "For all Parquet datasets, we create a sidecar file named **_common_metadata** in the root of the dataset. This file contains both the dataset-level and variable-level attributes.  \n",
    "The metadata can be retrieved below as a dictionary, and it will also be included in the pandas DataFrame when using the `get_data` method from the `GetAodn` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = aodn_dataset.get_metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92d38a",
   "metadata": {},
   "source": [
    "# Data Query and Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e53091",
   "metadata": {},
   "source": [
    "## Create a TIME and BoundingBox filter\n",
    "\n",
    "This cell loads a subset of the dataset based on a time range and a spatial bounding box. The result is returned as a pandas DataFrame, and basic information about its structure is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = aodn_dataset.get_data(date_start='2022-12-01', \n",
    "                           date_end='2023-01-01',\n",
    "                           lat_min=-34, \n",
    "                           lat_max=-28, \n",
    "                           lon_min=151, \n",
    "                           lon_max=160, \n",
    "                           )\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06bfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Subsetted Data as CSV\n",
    "\n",
    "# This cell downloads the filtered dataset as a ZIP-compressed CSV file.  \n",
    "# The CSV includes metadata at the top as commented lines, and a `FileLink` object is returned to allow downloading directly from the notebook.\n",
    "\n",
    "\n",
    "df.aodn.download_as_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a899c7",
   "metadata": {},
   "source": [
    "## Create a TIME and scalar/number filter\n",
    "\n",
    "This cell filters the dataset by time range and a scalar value (from a Parquet partition) using the `scalar_filter` argument.  \n",
    "This leverages Parquet partitioning to apply efficient, server-side filtering, which significantly speeds up data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = aodn_dataset.get_data(date_start='2006-07-12', \n",
    "                           date_end='2023-02-05',\n",
    "                           scalar_filter={'YOUR_PARTITION_KEY': 1901740})\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
