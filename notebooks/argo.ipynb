{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Toi10WECQdzJ"
   },
   "source": [
    "## Access ARGO Core data in Parquet\n",
    "\n",
    "A jupyter notebook to show how to access and plot ARGO Core data available as a [Parquet](https://parquet.apache.org) dataset on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TJf1YgjtQdzS"
   },
   "source": [
    "dataset_name = \"argo_core\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-FcvQ0UQdzW"
   },
   "source": [
    "## Install/Update packages and Load common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YB7J7Y8FQdzY",
    "outputId": "cd691404-a147-4c3d-96f1-55e8d2e66427"
   },
   "source": [
    "# only run once, then restart session and comment the next 3 lines\n",
    "!pip install s3fs -U\n",
    "!pip install pyarrow==16.0.0 -U\n",
    "!pip install zarr xarray[complete]\n",
    "!pip install pandas==2.2.2 -U"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeETrs8zQdza",
    "outputId": "5af2116e-0fd5-46a4-fc2c-d0a26d8fe24c"
   },
   "source": [
    "import requests\n",
    "import os\n",
    "if not os.path.exists('parquet_queries.py'):\n",
    "  print('Downloading parquet_queries.py')\n",
    "  url = 'https://raw.githubusercontent.com/aodn/aodn_cloud_optimised/main/notebooks/parquet_queries.py'\n",
    "  response = requests.get(url)\n",
    "  with open('parquet_queries.py', 'w') as f:\n",
    "      f.write(response.text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XzHxvlw8Qdzc"
   },
   "source": [
    "from parquet_queries import create_time_filter, create_bbox_filter, query_unique_value, plot_spatial_extent, get_spatial_extent, get_temporal_extent, get_schema_metadata\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.compute as pc"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AycdabTJQdze"
   },
   "source": [
    "## Location of the parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F5rE3sRuQdzf"
   },
   "source": [
    "BUCKET_OPTIMISED_DEFAULT=\"imos-data-lab-optimised\"\n",
    "dname = f\"s3://{BUCKET_OPTIMISED_DEFAULT}/cloud_optimised/cluster_testing/{dataset_name}.parquet/\"\n",
    "parquet_ds = pq.ParquetDataset(dname,partitioning='hive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IWdsh9NQdzh"
   },
   "source": [
    "# Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQWJBByXQdzh"
   },
   "source": [
    "## Get partition keys\n",
    "Partitioning in Parquet involves organising data files based on the values of one or more columns, known as partition keys. When data is written to Parquet files with partitioning enabled, the files are physically stored in a directory structure that reflects the partition keys. This directory structure makes it easier to retrieve and process specific subsets of data based on the partition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBVjvC1tQdzi",
    "outputId": "9896acb9-168b-4038-babc-ea4048e1d7fc"
   },
   "source": [
    "dataset = pds.dataset(dname, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "partition_keys = dataset.partitioning.schema\n",
    "print(partition_keys)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsP2odmAQdzk"
   },
   "source": [
    "## List unique partition values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIZF5O3cQdzl",
    "outputId": "abb16db6-2a9c-49d9-daf2-c00e1afb38ea"
   },
   "source": [
    "%%time\n",
    "unique_partition_value = query_unique_value(parquet_ds, 'PLATFORM_NUMBER')\n",
    "print(list(unique_partition_value)[0:2])  # showing a subset only"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6mnQtRWQdzm"
   },
   "source": [
    "## Visualise Spatial Extent of the dataset\n",
    "In this section, we're plotting the polygons where data exists. This helps then with creating a bounding box where there is data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "owI5Rx0kQdzn",
    "outputId": "d310299b-a056-4b71-f60f-05bfd25e00bb"
   },
   "source": [
    "plot_spatial_extent(parquet_ds)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqoUU0rSQdzn"
   },
   "source": [
    "## Get Temporal Extent of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8hIUrEiQdzo"
   },
   "source": [
    "Similary to the spatial extent, we're retrieving the minimum and maximum timestamp partition values of the dataset. This is not necessarely accurately representative of the TIME values, as the timestamp partition can be yearly/monthly... but is here to give an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLLrZLPRQdzo",
    "outputId": "4eaebf32-a48c-4ef3-f221-d3ed71f2e269"
   },
   "source": [
    "get_temporal_extent(parquet_ds)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkK9kuQdQdzp"
   },
   "source": [
    "## Read Metadata\n",
    "\n",
    "For all parquet dataset, we create a sidecar file in the root of the dataset named **_common_matadata**. This contains the variable attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYOh_AiLQdzp",
    "outputId": "d9578e1a-7a97-4c68-82b3-92a56e6cd765"
   },
   "source": [
    "# parquet_meta = pa.parquet.read_schema(os.path.join(dname + '_common_metadata'))  # parquet metadata\n",
    "get_schema_metadata(dname)  # schema metadata"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzmbSF4oQdzq"
   },
   "source": [
    "# Data Query and Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6QvVVLsQdzq"
   },
   "source": [
    "## Create a TIME and BoundingBox filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SQ8m5afVQdzq"
   },
   "source": [
    "filter_time = create_time_filter(parquet_ds, date_start='2018-12-01', date_end='2023-01-01')\n",
    "filter_geo = create_bbox_filter(parquet_ds, lat_min=-34, lat_max=-28, lon_min=151, lon_max=160)\n",
    "\n",
    "\n",
    "filter = filter_geo & filter_time"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYGO9pRwQdzq",
    "outputId": "c889f934-ded6-4c19-d3a3-49580d933bba"
   },
   "source": [
    "%%time\n",
    "# using pandas instead of pyarrow so that filters can directly be applied to the data, and not just the partition\n",
    "df = pd.read_parquet(dname, engine='pyarrow',filters=filter)\n",
    "df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "0VG-PwmIQdzs",
    "outputId": "fc029ab9-94e4-4c34-ba88-2e3241c5bf63"
   },
   "source": [
    "df.plot.scatter(x='TEMP_ADJUSTED', y='PSAL_ADJUSTED', c='PRES_ADJUSTED', marker='+', linestyle=\"None\", cmap='RdYlBu_r', title='Temperature for each location')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "geqOPVHIQdzt",
    "outputId": "b9fc1aea-cb14-417b-b692-5a76563163e7"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "filtered_df = df[df['PLATFORM_NUMBER'] == 5905506]\n",
    "\n",
    "# Get unique values of CYCLE_NUMBER\n",
    "unique_cycle_numbers = filtered_df['CYCLE_NUMBER'].unique()\n",
    "\n",
    "# Define a dictionary to map each unique CYCLE_NUMBER to a color\n",
    "color_mapping = {cycle_number: plt.cm.viridis_r(i / len(unique_cycle_numbers)) for i, cycle_number in enumerate(unique_cycle_numbers)}\n",
    "\n",
    "# Plot TEMP_ADJUSTED vs PRES_ADJUSTED with different colors for each line\n",
    "for cycle_number, color in color_mapping.items():\n",
    "    cycle_df = filtered_df[filtered_df['CYCLE_NUMBER'] == cycle_number]\n",
    "    plt.plot(cycle_df['TEMP_ADJUSTED'], cycle_df['PRES_ADJUSTED'], color=color, label=f'Cycle {cycle_number}')\n",
    "\n",
    "plt.xlabel('Temperature Adjusted')\n",
    "plt.ylabel('Pressure Adjusted')\n",
    "plt.title('Temperature vs Pressure')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Reverse the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psp_kOBFQdzt"
   },
   "source": [
    "## Create a TIME and scalar/number filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PtlD75nkQdzu"
   },
   "source": [
    "filter_time = create_time_filter(parquet_ds, date_start='2006-07-12', date_end='2023-02-05')\n",
    "\n",
    "expr_1 = pc.field('PLATFORM_NUMBER') == 1901740\n",
    "filter = expr_1 & filter_time"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQF8IgX9Qdzu",
    "outputId": "8930b940-58f5-449d-eb21-3d723bf6c0c8"
   },
   "source": [
    "%%time\n",
    "# using pandas instead of pyarrow so that filters can directly be applied to the data, and not just the partition\n",
    "df = pd.read_parquet(dname, engine='pyarrow',filters=filter)\n",
    "df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "dvO7kLp1Qdzu",
    "outputId": "3624827e-c2ca-4179-e20d-cdf8b2be6529"
   },
   "source": [
    "df.plot.scatter(x='TEMP_ADJUSTED', y='PSAL_ADJUSTED', c='PRES_ADJUSTED', marker='+', linestyle=\"None\", cmap='RdYlBu_r', title='Temperature for each location')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmq8RiggQdzv"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
