{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49bd3b8-7055-4157-9891-a8cefbf8541b",
   "metadata": {},
   "source": [
    "import requests\n",
    "import os\n",
    "if not os.path.exists('parquet_queries.py'):\n",
    "  print('Downloading parquet_queries.py')\n",
    "  url = 'https://raw.githubusercontent.com/aodn/aodn_cloud_optimised/main/notebooks/parquet_queries.py'\n",
    "  response = requests.get(url)\n",
    "  with open('parquet_queries.py', 'w') as f:\n",
    "      f.write(response.text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9008b0e2-7e1c-4ec3-ad99-b28a4ca80092",
   "metadata": {},
   "source": [
    "from parquet_queries import create_time_filter, create_bbox_filter, query_unique_value, plot_spatial_extent, get_spatial_extent, get_temporal_extent, get_schema_metadata\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.compute as pc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea9f5d0-a2a2-46a3-b553-f49500a334c7",
   "metadata": {},
   "source": [
    "import boto3\n",
    "BUCKET_OPTIMISED_DEFAULT=\"imos-data-lab-optimised\"\n",
    "\n",
    "def list_folders_with_parquet(bucket_name, prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Ensure the prefix ends with a slash to list \"folders\"\n",
    "    if not prefix.endswith('/'):\n",
    "        prefix += '/'\n",
    "\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "\n",
    "    folders = []\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        folder_path = prefix['Prefix']\n",
    "        if folder_path.endswith('.parquet/'):\n",
    "            # Extracting the folder name without the prefix\n",
    "            folder_name = folder_path[len(prefix) - 1:]\n",
    "            folders.append(folder_name)\n",
    "\n",
    "    return folders\n",
    "\n",
    "# Example usage\n",
    "bucket_name = BUCKET_OPTIMISED_DEFAULT\n",
    "prefix = 'parquet/loz_test'\n",
    "\n",
    "folders_with_parquet = list_folders_with_parquet(bucket_name, prefix)\n",
    "\n",
    "folders_with_parquet"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259550b7-2704-42c7-85b2-bcb177e78957",
   "metadata": {},
   "source": [
    "BUCKET_OPTIMISED_DEFAULT=\"imos-data-lab-optimised\"\n",
    "\n",
    "catalog = dict()\n",
    "for dataset in folders_with_parquet:\n",
    "    dname = f\"s3://{BUCKET_OPTIMISED_DEFAULT}/{dataset}\"\n",
    "    \n",
    "    metadata = get_schema_metadata(dname)  # schema metadata\n",
    "\n",
    "    path_parts = dataset.strip('/').split('/')\n",
    "    \n",
    "    # Get the last part of the path (folder name with extension)\n",
    "    last_folder_with_extension = path_parts[-1]\n",
    "    \n",
    "    # Remove the file extension\n",
    "    dataset_name = os.path.splitext(last_folder_with_extension)[0]\n",
    "        \n",
    "    catalog[dataset_name] = metadata"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7df4bf-d344-434a-ab51-897a44dc2d2f",
   "metadata": {},
   "source": [
    "catalog"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072a1a8c-deee-48fe-a966-75a6cb6dc943",
   "metadata": {},
   "source": [
    "!pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "709f3a69-f6a0-494a-a313-0c483d5a5f33",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_datasets_with_standard_name(data_dict, target_standard_name, threshold=80):\n",
    "    \"\"\"\n",
    "    Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "    that match or closely match the target_standard_name using fuzzy matching.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): The nested dictionary to search.\n",
    "        target_standard_name (str): The target value to match.\n",
    "        threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dataset names (main keys) where matches are found.\n",
    "    \"\"\"\n",
    "    matching_datasets = []\n",
    "\n",
    "    if not isinstance(data_dict, dict):\n",
    "        return matching_datasets  # Handle unexpected input data types\n",
    "\n",
    "    for dataset_name, attributes in data_dict.items():\n",
    "        if not isinstance(attributes, dict):\n",
    "            continue  # Skip non-dictionary attributes\n",
    "\n",
    "        for key, value in attributes.items():\n",
    "            if isinstance(value, dict) and 'standard_name' in value:\n",
    "                # Check for standard_name match using fuzzy matching\n",
    "                current_standard_name = value.get('standard_name', '')\n",
    "                similarity_score = fuzz.partial_ratio(target_standard_name.lower(), current_standard_name.lower())\n",
    "                if similarity_score >= threshold:\n",
    "                    matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "\n",
    "            # Recursively search deeper\n",
    "            matching_datasets.extend(find_datasets_with_standard_name(value, target_standard_name, threshold))\n",
    "\n",
    "    return list(set(matching_datasets)) \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "target_name = \"salinity\"\n",
    "result = find_datasets_with_standard_name(catalog, target_name)\n",
    "print(\"Dataset Name:\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24b8cf98-2d20-4a2e-8882-b497d41fa03a",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_datasets_with_attribute(data_dict, target_value, target_key='standard_name', threshold=80):\n",
    "    \"\"\"\n",
    "    Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "    that match or closely match the target_value using fuzzy matching.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): The nested dictionary to search.\n",
    "        target_value (str): The target value to match.\n",
    "        threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dataset names (main keys) where matches are found.\n",
    "    \"\"\"\n",
    "    matching_datasets = []\n",
    "\n",
    "    if not isinstance(data_dict, dict):\n",
    "        return matching_datasets  # Handle unexpected input data types\n",
    "\n",
    "    for dataset_name, attributes in data_dict.items():\n",
    "        if not isinstance(attributes, dict):\n",
    "            continue  # Skip non-dictionary attributes\n",
    "\n",
    "        for key, value in attributes.items():\n",
    "            if isinstance(value, dict) and target_key in value:\n",
    "                # Check for standard_name match using fuzzy matching\n",
    "                current_standard_name = value.get(target_key, '')\n",
    "                similarity_score = fuzz.partial_ratio(target_value.lower(), current_standard_name.lower())\n",
    "                if similarity_score >= threshold:\n",
    "                    matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "\n",
    "            # Recursively search deeper\n",
    "            matching_datasets.extend(find_datasets_with_attribute(value, target_value, threshold))\n",
    "\n",
    "    return list(set(matching_datasets)) \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "target_value = \"seconds\"\n",
    "result = find_datasets_with_attribute(catalog, target_value, target_key='units')\n",
    "print(\"Dataset Name:\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b33b2f6-76a3-4c4a-8281-2435ac06083a",
   "metadata": {},
   "source": [
    "class getAodn:\n",
    "    def __init__(self):#, **kwargs):\n",
    "        #self.test = kwargs.get('test', 'test')\n",
    "        self.bucket_name = BUCKET_OPTIMISED_DEFAULT\n",
    "        self.prefix = 'parquet/loz_test'\n",
    "\n",
    "        # initialise the class by calling the needed methods\n",
    "        self.metadata_catalog()\n",
    "\n",
    "    def metadata_catalog(self):\n",
    "        folders_with_parquet = list_folders_with_parquet(self.bucket_name, self.prefix)\n",
    "        \n",
    "        catalog = dict()\n",
    "        for dataset in folders_with_parquet:\n",
    "            dname = f\"s3://{self.bucket_name}/{dataset}\"\n",
    "            \n",
    "            metadata = get_schema_metadata(dname)  # schema metadata\n",
    "        \n",
    "            path_parts = dataset.strip('/').split('/')\n",
    "            \n",
    "            # Get the last part of the path (folder name with extension)\n",
    "            last_folder_with_extension = path_parts[-1]\n",
    "            \n",
    "            # Remove the file extension\n",
    "            dataset_name = os.path.splitext(last_folder_with_extension)[0]\n",
    "                \n",
    "            catalog[dataset_name] = metadata\n",
    "\n",
    "        self.catalog = catalog\n",
    "\n",
    "        return catalog\n",
    "    \n",
    "    def list_folders_with_parquet(self):\n",
    "        s3 = boto3.client('s3')\n",
    "        prefix = self.prefix\n",
    "        # Ensure the prefix ends with a slash to list \"folders\"\n",
    "        if not prefix.endswith('/'):\n",
    "            prefix += '/'\n",
    "    \n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix,\n",
    "            Delimiter='/'\n",
    "        )\n",
    "    \n",
    "        folders = []\n",
    "        for prefix in response.get('CommonPrefixes', []):\n",
    "            folder_path = prefix['Prefix']\n",
    "            if folder_path.endswith('.parquet/'):\n",
    "                # Extracting the folder name without the prefix\n",
    "                folder_name = folder_path[len(prefix) - 1:]\n",
    "                folders.append(folder_name)\n",
    "    \n",
    "        return folders\n",
    "\n",
    "    def find_datasets_attribute(self,attribute_value, attribute_key='standard_name', threshold=80):\n",
    "        \"\"\"\n",
    "        Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "        that match or closely match the attribute_key='standard_name', attribute_value using fuzzy matching.\n",
    "\n",
    "        Args:\n",
    "            data_dict (dict): The nested dictionary to search.\n",
    "            attribute_key='standard_name', attribute_value (str): The target value to match.\n",
    "            threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset names (main keys) where matches are found.\n",
    "        \"\"\"\n",
    "        matching_datasets = []\n",
    "\n",
    "        if not isinstance(self.catalog, dict):\n",
    "            return matching_datasets  # Handle unexpected input data types\n",
    "\n",
    "        for dataset_name, attributes in self.catalog.items():\n",
    "            if not isinstance(attributes, dict):\n",
    "                continue  # Skip non-dictionary attributes\n",
    "\n",
    "            for key, value in attributes.items():\n",
    "                if isinstance(value, dict) and attribute_key in value:\n",
    "                    # Check for standard_name match using fuzzy matching\n",
    "                    current_standard_name = value.get(attribute_key, '')\n",
    "                    similarity_score = fuzz.partial_ratio(attribute_value.lower(), current_standard_name.lower())\n",
    "                    if similarity_score >= threshold:\n",
    "                        matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "\n",
    "                # Recursively search deeper\n",
    "                matching_datasets.extend(self.find_datasets_attribute(value, attribute_value, attribute_key=attribute_key, threshold=threshold))\n",
    "\n",
    "        return list(set(matching_datasets)) "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15e25ee5-8ac4-46a2-b35f-1f46fe1538d2",
   "metadata": {},
   "source": [
    "aodn_instance = getAodn()\n",
    "\n",
    "aodn_instance.list_folders_with_parquet()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75527a75-342e-4953-9e59-eba81f81274f",
   "metadata": {},
   "source": [
    "aodn_instance.find_datasets_attribute('salinity')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59b358ca-392c-470e-9df9-72d90128c009",
   "metadata": {},
   "source": [
    "aodn_instance.find_datasets_attribute('wave')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3244b418-4567-4d1b-bd9b-1e5df7c4ee00",
   "metadata": {},
   "source": [
    "aodn_instance.find_datasets_with_standard_name('salinity')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "377eb475-fe23-421c-9461-a77fc588a9be",
   "metadata": {},
   "source": [
    "class getAodn:\n",
    "    def __init__(self):#, **kwargs):\n",
    "        #self.test = kwargs.get('test', 'test')\n",
    "        self.bucket_name = BUCKET_OPTIMISED_DEFAULT\n",
    "        self.prefix = 'parquet/loz_test'\n",
    "\n",
    "        # initialise the class by calling the needed methods\n",
    "        self.catalog = self.metadata_catalog()\n",
    "\n",
    "    def metadata_catalog(self):\n",
    "        folders_with_parquet = list_folders_with_parquet(self.bucket_name, self.prefix)\n",
    "        \n",
    "        catalog = dict()\n",
    "        for dataset in folders_with_parquet:\n",
    "            dname = f\"s3://{self.bucket_name}/{dataset}\"\n",
    "            \n",
    "            metadata = get_schema_metadata(dname)  # schema metadata\n",
    "        \n",
    "            path_parts = dataset.strip('/').split('/')\n",
    "            \n",
    "            # Get the last part of the path (folder name with extension)\n",
    "            last_folder_with_extension = path_parts[-1]\n",
    "            \n",
    "            # Remove the file extension\n",
    "            dataset_name = os.path.splitext(last_folder_with_extension)[0]\n",
    "                \n",
    "            catalog[dataset_name] = metadata\n",
    "\n",
    "        return catalog\n",
    "    \n",
    "    def list_folders_with_parquet(self):\n",
    "        s3 = boto3.client('s3')\n",
    "        prefix = self.prefix\n",
    "        # Ensure the prefix ends with a slash to list \"folders\"\n",
    "        if not prefix.endswith('/'):\n",
    "            prefix += '/'\n",
    "    \n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix,\n",
    "            Delimiter='/'\n",
    "        )\n",
    "    \n",
    "        folders = []\n",
    "        for prefix in response.get('CommonPrefixes', []):\n",
    "            folder_path = prefix['Prefix']\n",
    "            if folder_path.endswith('.parquet/'):\n",
    "                # Extracting the folder name without the prefix\n",
    "                folder_name = folder_path[len(prefix) - 1:]\n",
    "                folders.append(folder_name)\n",
    "    \n",
    "        return folders\n",
    "\n",
    "    def find_datasets_with_standard_name(self, target_standard_name, threshold=80):\n",
    "        \"\"\"\n",
    "        Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "        that match or closely match the target_standard_name using fuzzy matching.\n",
    "\n",
    "        Args:\n",
    "            data_dict (dict): The nested dictionary to search.\n",
    "            target_standard_name (str): The target value to match.\n",
    "            threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dataset names (main keys) where matches are found.\n",
    "        \"\"\"\n",
    "        matching_datasets = []\n",
    "\n",
    "        if not isinstance(self.catalog, dict):\n",
    "            return matching_datasets  # Handle unexpected input data types\n",
    "\n",
    "        for dataset_name, attributes in self.catalog.items():\n",
    "            if not isinstance(attributes, dict):\n",
    "                continue  # Skip non-dictionary attributes\n",
    "\n",
    "            for key, value in attributes.items():\n",
    "                if isinstance(value, dict) and 'standard_name' in value:\n",
    "                    # Check for standard_name match using fuzzy matching\n",
    "                    current_standard_name = value.get('standard_name', '')\n",
    "                    similarity_score = fuzz.partial_ratio(target_standard_name.lower(), current_standard_name.lower())\n",
    "                    if similarity_score >= threshold:\n",
    "                        matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "\n",
    "                # Recursively search deeper\n",
    "                matching_datasets.extend(find_datasets_with_standard_name(value, target_standard_name, threshold))\n",
    "\n",
    "        return list(set(matching_datasets))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e04a5043-32f2-4e42-bb5f-bbded0124812",
   "metadata": {},
   "source": [
    "aodn_instance = getAodn()\n",
    "aodn_instance.list_folders_with_parquet()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb9b5059-6b23-4c75-bd15-c5930a9651dd",
   "metadata": {},
   "source": [
    "aodn_instance.find_datasets_with_standard_name('temp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c2bdb8db-1d47-4192-8f92-8d86b5fac723",
   "metadata": {},
   "source": [
    "class getAodn:\n",
    "    def __init__(self):#, **kwargs):\n",
    "        #self.test = kwargs.get('test', 'test')\n",
    "        self.bucket_name = BUCKET_OPTIMISED_DEFAULT\n",
    "        self.prefix = 'parquet/loz_test'\n",
    "\n",
    "        # initialise the class by calling the needed methods\n",
    "        self.catalog = self.metadata_catalog()\n",
    "\n",
    "    def metadata_catalog(self):\n",
    "        folders_with_parquet = list_folders_with_parquet(self.bucket_name, self.prefix)\n",
    "        \n",
    "        catalog = dict()\n",
    "        for dataset in folders_with_parquet:\n",
    "            dname = f\"s3://{self.bucket_name}/{dataset}\"\n",
    "            \n",
    "            metadata = get_schema_metadata(dname)  # schema metadata\n",
    "        \n",
    "            path_parts = dataset.strip('/').split('/')\n",
    "            \n",
    "            # Get the last part of the path (folder name with extension)\n",
    "            last_folder_with_extension = path_parts[-1]\n",
    "            \n",
    "            # Remove the file extension\n",
    "            dataset_name = os.path.splitext(last_folder_with_extension)[0]\n",
    "                \n",
    "            catalog[dataset_name] = metadata\n",
    "\n",
    "        return catalog\n",
    "    \n",
    "    def list_folders_with_parquet(self):\n",
    "        s3 = boto3.client('s3')\n",
    "        prefix = self.prefix\n",
    "        # Ensure the prefix ends with a slash to list \"folders\"\n",
    "        if not prefix.endswith('/'):\n",
    "            prefix += '/'\n",
    "    \n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix,\n",
    "            Delimiter='/'\n",
    "        )\n",
    "    \n",
    "        folders = []\n",
    "        for prefix in response.get('CommonPrefixes', []):\n",
    "            folder_path = prefix['Prefix']\n",
    "            if folder_path.endswith('.parquet/'):\n",
    "                # Extracting the folder name without the prefix\n",
    "                folder_name = folder_path[len(prefix) - 1:]\n",
    "                folders.append(folder_name)\n",
    "    \n",
    "        return folders\n",
    "\n",
    "    def find_datasets_with_attribute(self, target_value, target_key='standard_name',data_dict=None, threshold=80):\n",
    "        \"\"\"\n",
    "        Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "        that match or closely match the target_value using fuzzy matching.\n",
    "    \n",
    "        Args:\n",
    "            data_dict (dict): The nested dictionary to search.\n",
    "            target_value (str): The target value to match.\n",
    "            threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of dataset names (main keys) where matches are found.\n",
    "        \"\"\"\n",
    "        matching_datasets = []\n",
    "        # https://stackoverflow.com/questions/56535948/python-why-cant-you-use-a-self-variable-as-an-optional-argument-in-a-method\n",
    "        if data_dict == None:\n",
    "            data_dict = self.catalog\n",
    "    \n",
    "        if not isinstance(data_dict, dict):\n",
    "            return matching_datasets  # Handle unexpected input data types\n",
    "    \n",
    "        for dataset_name, attributes in data_dict.items():\n",
    "            if not isinstance(attributes, dict):\n",
    "                continue  # Skip non-dictionary attributes\n",
    "    \n",
    "            for key, value in attributes.items():\n",
    "                if isinstance(value, dict) and target_key in value:\n",
    "                    # Check for standard_name match using fuzzy matching\n",
    "                    current_standard_name = value.get(target_key, '')\n",
    "                    similarity_score = fuzz.partial_ratio(target_value.lower(), current_standard_name.lower())\n",
    "                    if similarity_score >= threshold:\n",
    "                        matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "    \n",
    "                # Recursively search deeper\n",
    "                matching_datasets.extend(self.find_datasets_with_attribute(value, target_value, threshold))\n",
    "    \n",
    "        return list(set(matching_datasets))\n",
    "\n",
    "    class get_dataset(getAodn):\n",
    "        def __init__(self, dataset_name):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dname = f\"s3://{self.bucket_name}/{self.prefix}/{dataset_name}.parquet/\"\n",
    "            self.parquet_ds = pq.ParquetDataset(dname, partitioning='hive')\n",
    "\n",
    "        def partition_keys_list(self):\n",
    "            dataset = pds.dataset(self.dname, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "            partition_keys = dataset.partitioning.schema\n",
    "            return partition_keys\n",
    "\n",
    "        def spatial_extent(self):\n",
    "            return plot_spatial_extent(self.parquet_ds)\n",
    "\n",
    "        def get_temporal_extent(self):\n",
    "            return get_temporal_extent(self.parquet_ds)\n",
    "\n",
    "        def get_data(self, date_start=None, date_end=None, lat_min=None, lat_max=None, lon_min=None, lon_max=None):\n",
    "            filter_time = create_time_filter(self.parquet_ds, date_start=date_start, date_end=date_end)\n",
    "            filter_geo = create_bbox_filter(self.parquet_ds, lat_min=lat_min, lat_max=lat_max, lon_min=lon_min, lon_max=lon_max)\n",
    "\n",
    "\n",
    "            filter = filter_geo & filter_time\n",
    "            df = pd.read_parquet(self.dname, engine='pyarrow',filters=filter)\n",
    "            return df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "543a4f6c-0dec-4a3f-9cfc-09446d7ff207",
   "metadata": {},
   "source": [
    "aodn_instance = getAodn()\n",
    "#catalog = aodn_instance.metadata_catalog()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "48847f81-d4ac-4bc1-8faf-f06d80beeded",
   "metadata": {},
   "source": [
    "aodn_instance.find_datasets_with_attribute('latitude',target_key='standard_name' )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "310d893b-8fe0-46c1-8de9-61a3a1f93cc7",
   "metadata": {},
   "source": [
    "aodn_instance.get_dataset('soop_xbt_nrt').partition_keys_list()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d7a6c3b1-d07d-4e75-9d53-fac4f264cd3d",
   "metadata": {},
   "source": [
    "aodn_instance.get_dataset('soop_xbt_nrt').spatial_extent()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7ac4d97a-2d9f-4aaa-88b6-06557a03527a",
   "metadata": {},
   "source": [
    "aodn_instance.get_dataset('soop_xbt_nrt').get_temporal_extent()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "705a7cfa-e0b6-4732-a655-3f7122ac9fb6",
   "metadata": {},
   "source": [
    "data= aodn_instance.get_dataset('soop_xbt_nrt').get_data(date_start='2023-01-31 10:14:00', \n",
    "                                                   date_end='2024-02-01 07:50:00',\n",
    "                                                   lat_min=-34, lat_max=-32, lon_min=150, lon_max=155)\n",
    "                                                   \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "87553060-85df-49c7-ac66-fd01ac56e59a",
   "metadata": {},
   "source": [
    "data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cc513ccd-4014-4f30-9c6a-394cd2cb7bc4",
   "metadata": {},
   "source": [
    "class getAodn:\n",
    "    def __init__(self):\n",
    "        self.bucket_name = BUCKET_OPTIMISED_DEFAULT\n",
    "        self.prefix = 'parquet/loz_test'\n",
    "\n",
    "    class metadata(getAodn):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # initialise the class by calling the needed methods\n",
    "            self.catalog = self.metadata_catalog()\n",
    "            \n",
    "        def metadata_catalog(self):\n",
    "            folders_with_parquet = list_folders_with_parquet(self.bucket_name, self.prefix)\n",
    "            \n",
    "            catalog = dict()\n",
    "            for dataset in folders_with_parquet:\n",
    "                dname = f\"s3://{self.bucket_name}/{dataset}\"\n",
    "                \n",
    "                metadata = get_schema_metadata(dname)  # schema metadata\n",
    "            \n",
    "                path_parts = dataset.strip('/').split('/')\n",
    "                \n",
    "                # Get the last part of the path (folder name with extension)\n",
    "                last_folder_with_extension = path_parts[-1]\n",
    "                \n",
    "                # Remove the file extension\n",
    "                dataset_name = os.path.splitext(last_folder_with_extension)[0]\n",
    "                    \n",
    "                catalog[dataset_name] = metadata\n",
    "    \n",
    "            return catalog\n",
    "        \n",
    "        def list_folders_with_parquet(self):\n",
    "            s3 = boto3.client('s3')\n",
    "            prefix = self.prefix\n",
    "            # Ensure the prefix ends with a slash to list \"folders\"\n",
    "            if not prefix.endswith('/'):\n",
    "                prefix += '/'\n",
    "        \n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=self.bucket_name,\n",
    "                Prefix=prefix,\n",
    "                Delimiter='/'\n",
    "            )\n",
    "        \n",
    "            folders = []\n",
    "            for prefix in response.get('CommonPrefixes', []):\n",
    "                folder_path = prefix['Prefix']\n",
    "                if folder_path.endswith('.parquet/'):\n",
    "                    # Extracting the folder name without the prefix\n",
    "                    folder_name = folder_path[len(prefix) - 1:]\n",
    "                    folders.append(folder_name)\n",
    "        \n",
    "            return folders\n",
    "    \n",
    "        def find_datasets_with_attribute(self, target_value, target_key='standard_name',data_dict=None, threshold=80):\n",
    "            \"\"\"\n",
    "            Recursively search a nested dictionary for 'standard_name' keys with values\n",
    "            that match or closely match the target_value using fuzzy matching.\n",
    "        \n",
    "            Args:\n",
    "                data_dict (dict): The nested dictionary to search.\n",
    "                target_value (str): The target value to match.\n",
    "                threshold (int): The minimum similarity threshold for fuzzy matching (default: 80).\n",
    "        \n",
    "            Returns:\n",
    "                list: A list of dataset names (main keys) where matches are found.\n",
    "            \"\"\"\n",
    "            matching_datasets = []\n",
    "            # https://stackoverflow.com/questions/56535948/python-why-cant-you-use-a-self-variable-as-an-optional-argument-in-a-method\n",
    "            if data_dict == None:\n",
    "                data_dict = self.catalog\n",
    "        \n",
    "            if not isinstance(data_dict, dict):\n",
    "                return matching_datasets  # Handle unexpected input data types\n",
    "        \n",
    "            for dataset_name, attributes in data_dict.items():\n",
    "                if not isinstance(attributes, dict):\n",
    "                    continue  # Skip non-dictionary attributes\n",
    "        \n",
    "                for key, value in attributes.items():\n",
    "                    if isinstance(value, dict) and target_key in value:\n",
    "                        # Check for standard_name match using fuzzy matching\n",
    "                        current_standard_name = value.get(target_key, '')\n",
    "                        similarity_score = fuzz.partial_ratio(target_value.lower(), current_standard_name.lower())\n",
    "                        if similarity_score >= threshold:\n",
    "                            matching_datasets.append(dataset_name)  # Add dataset name to list\n",
    "        \n",
    "                    # Recursively search deeper\n",
    "                    matching_datasets.extend(self.find_datasets_with_attribute(value, target_value, threshold))\n",
    "        \n",
    "            return list(set(matching_datasets))\n",
    "\n",
    "    class get_dataset(getAodn):\n",
    "        def __init__(self, dataset_name):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dname = f\"s3://{self.bucket_name}/{self.prefix}/{dataset_name}.parquet/\"\n",
    "            self.parquet_ds = pq.ParquetDataset(dname, partitioning='hive')\n",
    "\n",
    "        def partition_keys_list(self):\n",
    "            dataset = pds.dataset(self.dname, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "            partition_keys = dataset.partitioning.schema\n",
    "            return partition_keys\n",
    "\n",
    "        def spatial_extent(self):\n",
    "            return plot_spatial_extent(self.parquet_ds)\n",
    "\n",
    "        def get_temporal_extent(self):\n",
    "            return get_temporal_extent(self.parquet_ds)\n",
    "\n",
    "        def get_data(self, date_start=None, date_end=None, lat_min=None, lat_max=None, lon_min=None, lon_max=None):\n",
    "            filter_time = create_time_filter(self.parquet_ds, date_start=date_start, date_end=date_end)\n",
    "            filter_geo = create_bbox_filter(self.parquet_ds, lat_min=lat_min, lat_max=lat_max, lon_min=lon_min, lon_max=lon_max)\n",
    "\n",
    "\n",
    "            filter = filter_geo & filter_time\n",
    "            df = pd.read_parquet(self.dname, engine='pyarrow',filters=filter)\n",
    "            return df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fa7e8981-448a-4f3d-a5bc-0c6ced87c357",
   "metadata": {},
   "source": [
    "aodn_instance = getAodn()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "485b8696-fd87-4484-8eac-6fe61ce6190b",
   "metadata": {},
   "source": [
    "aodn_instance.metadata().metadata_catalog()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c8f0120a-66b8-4462-9934-0f66dfbc4678",
   "metadata": {},
   "source": [
    "aodn_instance.metadata().find_datasets_with_attribute('temp',target_key='standard_name' )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "df51e8cd-cb70-4417-9a09-d1b2bdf92ab2",
   "metadata": {},
   "source": [
    "aodn_instance.get_dataset('soop_xbt_nrt').get_temporal_extent()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a406c5b9-3cb5-4617-a762-53e75603fda8",
   "metadata": {},
   "source": [
    "df = aodn_instance.get_dataset('soop_xbt_nrt').get_data(date_start='2023-01-31 10:14:00', \n",
    "                                                   date_end='2024-02-01 07:50:00',\n",
    "                                                   lat_min=-34, lat_max=-32, lon_min=150, lon_max=155)\n",
    "                                                   \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b226900d-b512-4e94-92ab-5d5e302e32e1",
   "metadata": {},
   "source": [
    "df[df['TEMP_quality_control'] == 1].sort_values('TIME').plot.scatter(x='TEMP', y='DEPTH', c='TIME', \n",
    "                                                                     xlabel=metadata['TEMP']['standard_name'],\n",
    "                                                                     ylabel=metadata['DEPTH']['standard_name'],\n",
    "                                                                     cmap='RdYlBu_r', marker='.', linestyle=\"None\").invert_yaxis()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48852e-38d2-4328-ba1b-66655c25c093",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
